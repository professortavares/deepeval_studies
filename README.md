# DeepEval studies
Repository for studies on automated evaluation of LLMs using DeepEval

## Installation

1. Create a virtual environment (optional but recommended):
```bash
python -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`
```  

2. Install the required packages:
```bash
pip install -r requirements.txt
```
3. Create a .env file in the root directory and add your OpenAI API key:
```
OPENAI_API_KEY=your_openai_api_key_here
```

## References - Papers about LLM-as-a-judge


| Paper / Articles                                                                        | Link                                                 |
|-----------------------------------------------------------------------------------------|------------------------------------------------------|
| Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena                                  | [arXiv:2306.05685](https://arxiv.org/abs/2306.05685) |
| Can Large Language Models Be an Alternative to Human Evaluations?                       | [arXiv:2305.01937](https://arxiv.org/abs/2305.01937) |
| GPTScore: Evaluate as You Desire                                                        | [arXiv:2302.04166](https://arxiv.org/abs/2302.04166) |
| Is ChatGPT a Good NLG Evaluator? A Preliminary Study                                    | [arXiv:2303.04048](https://arxiv.org/abs/2303.04048) |
| ChatGPT as a Factual Inconsistency Evaluator for Text Summarization                     | [arXiv:2303.15621](https://arxiv.org/abs/2303.15621) | 
| Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models | [arXiv:2404.18796](https://arxiv.org/abs/2404.18796) |
| G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment                          | [arXiv:2303.16634](https://arxiv.org/abs/2303.16634) |
| Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form Text      | [arXiv:2408.09235](https://arxiv.org/abs/2408.09235) |
| xFinder: Large Language Models as Automated Evaluators for Reliable Evaluation          | https://openreview.net/forum?id=7UqQJUKaLM           |
| Large Language Models as Evaluators for Recommendation Explanations                     | https://dl.acm.org/doi/10.1145/3640457.3688075       |
| Using LLMs for Evaluation                                                               | https://cameronrwolfe.substack.com/p/llm-as-a-judge  | 

